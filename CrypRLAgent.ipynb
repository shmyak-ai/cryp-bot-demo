{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Reinforcement learning cryptocurrency trading bot"]},{"cell_type":"markdown","metadata":{"id":"J7V7WnLvVzyO"},"source":["   <a href=\"https://colab.research.google.com/github/shmyak-ai/cryp-bot-demo/blob/main/CrypRLAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{},"source":["Disclaimer:\n","This article is for information purposes only.\n","It is not intended to be investment advice. Seek a duly licensed professional for investment advice."]},{"cell_type":"markdown","metadata":{},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{},"source":["There are many paid and open source cryptocurrency trading bots on the Internet.\n","Most of them are not fully automated, you need to configure parameters, choose or develop a strategy, etc. in order to use them.\n","These bots can help users trade better, but cannot operate independently.\n","Another problem with these bots is that they are rule-based (or heuristic), they apply some hard-written strategies to achieve profits.\n","This makes them sub-optimal, since no one knows if there are better strategies or not.\n","There is a group of algorithms that find the optimal solution, it is called reinforcement learning."]},{"cell_type":"markdown","metadata":{},"source":["There is no purpose to describe reinforcement algorithms in this article.\n","One can find a nice overview for example [here](https://lilianweng.github.io/posts/2018-02-19-rl-overview/).\n","In short, reinforcement learning requires two parts:\n","1. Environment.\n","One can consider it as a function, where input parameters are actions and output parameters are a state and a reward.\n","For example, a crypto trading market can be an environment.\n","This environment at each time step receives actions from traders and produces a new state (new cryptocurrency prices) and rewards for traders (their profit or loss).\n","2. Agent.\n","The main part of an agent is a function, which receives states (and sometimes rewards) and produces actions.\n","Nowadays a deep neural net is the most popular choice to approximate this function.\n","\n","Reinforcement learning algorithm teaches an agent to make actions that yield more reward.\n","In other words, a reinforcement learning algorithm optimizes a function (a deep neural net) with a constraint of maximazing rewards."]},{"cell_type":"markdown","metadata":{},"source":["Let's implement an example of a bot trader with reinforcement learning in Python.\n","The code can be run immediately in Colab at https://github.com/shmyak-ai/cryp-bot-demo/blob/main/CrypRLAgent.ipynb."]},{"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"markdown","metadata":{},"source":["Along with Python Standard Library and popular packages for data analisys, manipulation and visualization (Numpy, Pandas, Matplotlib, Seaborn), we will use some python packages specific to reinforcement learning:\n","- [Gym](https://www.gymlibrary.ml/) is a standard API for reinforcement learning environments.\n","- [RLlib](https://docs.ray.io/en/latest/rllib/index.html) is an open-source library for reinforcement learning."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23066,"status":"ok","timestamp":1653803187971,"user":{"displayName":"shamil yakubov","userId":"14689336937540158458"},"user_tz":-240},"id":"uTGdVd41K5qx","outputId":"e287213d-a549-4a6f-cb9b-256cd4b6c02c"},"outputs":[],"source":["if 'google.colab' in str(get_ipython()):\n","  print('Running on CoLab')\n","  %pip install 'ray[default,rllib]' &>/dev/null || echo \"Install failed!\"\n","else:\n","  print('Not running on CoLab')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8558,"status":"ok","timestamp":1653803196520,"user":{"displayName":"shamil yakubov","userId":"14689336937540158458"},"user_tz":-240},"id":"rwHGDuwjJaVT"},"outputs":[],"source":["import os\n","import random\n","import urllib.request\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import gym\n","from gym import spaces\n","from ray.rllib.agents import ppo\n","from ray.rllib.env.env_context import EnvContext\n","\n","sns.set_style(\"darkgrid\")"]},{"cell_type":"markdown","metadata":{"id":"8SQGI3BhJaVd"},"source":["## Data preparation"]},{"cell_type":"markdown","metadata":{},"source":["A trading environment requires some data to represent a state of the market.\n","A state should be informative enough to allow an agent to make a reasonable action.\n","Traders widely use [candlestick data](https://en.wikipedia.org/wiki/Candlestick_chart) to make decisions, so it should fit our needs as well.\n","I have downloaded 1 minute candlestick data from [Binance](https://www.binance.com/en) for pair BTC-USDT.\n","BTC is the most popular cryptocurrency - bitcoin.\n","USDT is the most popular stabel coin, which is approximately equivalent to USD.\n","There is nice python script available [here](https://github.com/gosuto-ai/candlestick_retriever) to download data from Binance.\n","Probably 1 minute candlestick data is not enough to make good decisions, since it does not contain any data about previous time steps.\n","We can therefore make candlesticks data for other periods from the data we have.\n","Also we need to normalize data, here we will take a short period for training (from 22-01-31 to 22-03-01 after data preparation) so it is possible to use mean and standard deviation for this period for data normalization."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1653803196524,"user":{"displayName":"shamil yakubov","userId":"14689336937540158458"},"user_tz":-240},"id":"jjcC6sduJaVe"},"outputs":[],"source":["PERIODS = ['3min', '5min', '15min', '30min', '1h', '2h', '4h', '6h', '8h', '12h', '1d', '3d', '7d', '30d']\n","START_DATE = '2022-01-01'\n","INITIAL_DATE = '2022-01-31'\n","FINISH_DATE = '2022-03-01'"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5505,"status":"ok","timestamp":1653803202003,"user":{"displayName":"shamil yakubov","userId":"14689336937540158458"},"user_tz":-240},"id":"0VA2EHP7JaVe"},"outputs":[],"source":["try:\n","    df = pd.read_parquet('data/BTC-USDT.parquet')\n","except FileNotFoundError:\n","    os.makedirs('data', exist_ok=True)\n","    urllib.request.urlretrieve('https://github.com/shmyak-ai/cryp-bot-demo/raw/main/data/BTC-USDT.parquet', \n","                               'data/BTC-USDT.parquet')\n","    df = pd.read_parquet('data/BTC-USDT.parquet')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":351},"executionInfo":{"elapsed":878,"status":"ok","timestamp":1653803202871,"user":{"displayName":"shamil yakubov","userId":"14689336937540158458"},"user_tz":-240},"id":"X6Oz455GJaVf","outputId":"7bdb3015-4049-4c7e-bca7-53a68330f293"},"outputs":[],"source":["fig, ax = plt.subplots(1, 1, figsize=(14, 5))\n","ax.plot(df[START_DATE:FINISH_DATE].index, df[START_DATE:FINISH_DATE]['close'])\n","ax.set_xlabel(\"Time\")\n","ax.set_ylabel(\"Price\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":83827,"status":"ok","timestamp":1653803286680,"user":{"displayName":"shamil yakubov","userId":"14689336937540158458"},"user_tz":-240},"id":"fehOsfxkJaVg"},"outputs":[],"source":["df_train = df[START_DATE:FINISH_DATE][['open', 'high', 'low', 'close']].copy()\n","df_train[['open', 'high', 'low', 'close']] = df_train[['open', 'high', 'low', 'close']] - df_train[['open', 'high', 'low', 'close']].mean().mean()\n","df_train[['open', 'high', 'low', 'close']] = df_train[['open', 'high', 'low', 'close']] / df_train[['open', 'high', 'low', 'close']].std().mean()\n","df_train['price'] = df[START_DATE:FINISH_DATE]['close']\n","\n","for period in PERIODS:\n","    df_train['open' + period] = df_train['open'].rolling(period).agg(lambda rows: rows[0])\n","    df_train['high' + period] = df_train['high'].rolling(period).max()\n","    df_train['low' + period] = df_train['low'].rolling(period).min()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1653803286682,"user":{"displayName":"shamil yakubov","userId":"14689336937540158458"},"user_tz":-240},"id":"-Ezk9k-CJaVh"},"outputs":[],"source":["df_train = df_train[INITIAL_DATE:]"]},{"cell_type":"markdown","metadata":{"id":"YJq0kaU6JaVZ"},"source":["## Environment"]},{"cell_type":"markdown","metadata":{},"source":["Let's implement a Gym environment, which RLlib can use.\n","A RLlib ready environment consist of three main parts, a constructor, a method to reset an environment, and a method for step.\n","- A constructor should specify action and observation spaces.\n","For the action space we choose a continuous interval [-1, 1], where -1 means sell everything up to the base cash equivalent, 1 means buy an asset up to the base cash equivalent.\n","So the agent will be able to choose the amount of asset it wants to buy or sell.\n","As a base cash we will use 10000 usdt, and a trading asset is bitcoin.\n","Thus, for example, action -0.5 means sell bitcoins equivalent to 5000 usdt.\n","Observation space is a number of records in the observations array for a time step, which consist of prepared candlestick data, plus 2 records for amounts of usdt and bitcoin.\n","- A reset should return an initial state of environment. \n","The real crypto trade market operates continuosly through days, monthes, etc.\n","But we will limit our reinforcement learning environment 'episode' with 2000 steps.\n","1440 minutes equals a day, so 2000 steps is approximately a day and a half.\n","The agent will learn to get as much as possible rewards for this period.\n","After each reset we will sample randomly an initial step, and than the environment will continue for 2000 steps until the end.\n","The initial state of environment is the observation array for the initial step.\n","- A step should return the current state, a reward at the current step, and if environment is done.\n","The current state is the observation array for the current step.\n","The reward is difference between the current total asset with the previous step total asset.\n","Total asset equals a sum of usdt and bitcoin usdt equivalent for the current step price.\n","Environment is done on the 2000 step as was mentioned previously."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1653803196523,"user":{"displayName":"shamil yakubov","userId":"14689336937540158458"},"user_tz":-240},"id":"wmZ3QwN9JaVb"},"outputs":[],"source":["def prepare_dict(df):\n","    price_array = df['price'].to_numpy(dtype=np.float32)[:, np.newaxis]\n","    df = df.drop(columns=['price'])\n","    obs_array = df.to_numpy(dtype=np.float32)\n","    data_dictionary = {'price_array': price_array, 'observations': obs_array}\n","    return data_dictionary\n","\n","\n","class CryptoEnv(gym.Env):\n","    def __init__(self, config: EnvContext):\n","        self._price_array = config['price_array']\n","        self._observations = config['observations']\n","\n","        self._base_cash = config['initial_capital']\n","        self._cash_usd = None\n","        self._stocks_usd = None\n","        self._stocks = None\n","        self._total_asset = None\n","        self._initial_total_asset = None\n","\n","        self._time_step = None\n","        self._initial_step = None\n","        self._max_steps = config['max_steps']\n","        self._final_step = None\n","        self._upper_bound_step = self._price_array.shape[0] - self._max_steps - 1\n","\n","        self._gamma = config['gamma']\n","        self._gamma_return = None\n","\n","        self._action_dim = self._price_array.shape[1]\n","        # buy or sell up to the base cash equivalent(usd)\n","        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(self._action_dim,), dtype=np.float32)\n","        # cash + stocks + observations\n","        self._state_dim = 2 + self._observations.shape[1]\n","        self.observation_space = spaces.Box(low=-5.0, high=5.0, shape=(self._state_dim,), dtype=np.float32)\n","\n","        self._state = None\n","        self._episode_ended = None\n","\n","    def reset(self):\n","        self._time_step = self._initial_step = random.randint(0, self._upper_bound_step)\n","        self._final_step = self._initial_step + self._max_steps\n","        self._cash_usd = random.random() * self._base_cash\n","        self._stocks_usd = random.random() * self._base_cash\n","        self._stocks = self._stocks_usd / self._price_array[self._time_step][0]\n","        self._total_asset = self._initial_total_asset = self._cash_usd + self._stocks_usd\n","        self._gamma_return = 0.0\n","\n","        self._state = self._get_state()\n","        self._episode_ended = False\n","        return self._state\n","\n","    def step(self, action):\n","        self._time_step += 1\n","        price = self._price_array[self._time_step][0]\n","        self._stocks_usd = self._stocks * price\n","        if action[0] < 0 and price > 0:  # sell\n","            sell_shares_usd = min(self._base_cash * -action[0], self._stocks_usd)\n","            self._stocks_usd -= sell_shares_usd\n","            self._cash_usd += sell_shares_usd\n","        elif action[0] > 0 and price > 0:  # buy\n","            money_to_spend = min(self._base_cash * action[0], self._cash_usd)\n","            self._stocks_usd += money_to_spend\n","            self._cash_usd -= money_to_spend\n","        self._stocks = self._stocks_usd / price\n","\n","        self._episode_ended = self._time_step == self._final_step\n","        self._state = self._get_state()\n","        next_total_asset = self._cash_usd + self._stocks_usd\n","\n","        reward = (next_total_asset - self._total_asset) / self._base_cash\n","        self._total_asset = next_total_asset\n","        self._gamma_return = self._gamma_return * self._gamma + reward\n","        if self._episode_ended:\n","            reward = self._gamma_return\n","            return self._state, reward, True, self._get_info()\n","        else:\n","            return self._state, reward, False, self._get_info()\n","\n","    def _get_state(self):\n","        state = np.hstack(((self._cash_usd - self._base_cash) / self._base_cash, \n","                           (self._stocks_usd - self._base_cash) / self._base_cash))\n","        observation = self._observations[self._time_step]\n","        state = np.hstack((state, observation)).astype(np.float32)\n","        return state\n","\n","    def _get_info(self):\n","        return {\"Initial step\": self._initial_step, \n","                \"Final step\": self._final_step, \n","                \"Initial total asset\": self._initial_total_asset, \n","                \"Final total asset\": self._total_asset, \n","                \"Gamma return\": self._gamma_return}"]},{"cell_type":"markdown","metadata":{"id":"y5GIpAI7JaVh"},"source":["## Training agent"]},{"cell_type":"markdown","metadata":{},"source":["We will use [RLlib PPO](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#ppo) algorithm for reinforcement learning.\n","This algorithm is a robust choice for many tasks.\n","In a configuration we should provide the environment, which we have implemented before, and specify some other parameters one can see below. \n","The training will continue for 200 iterations in total.\n","\n","One PPO iteration consist of two phases: experience gathering and training.\n","Before start RLlib PPO will initialize a reinforcement learning agent with a dense (256, 256) neural net. \n","Neural net takes a state (candlestick observations plus amount of usdt and bitcoin for the current step) as input, and outputs, roughly, 2 quantities: an action (a value from -1 to 1) and a value that estimates a sum of future rewards.\n","- During experience gathering the algorithm will collect 'experience trajectories' using the neural net as an actor (a trader in our case) in the provided environment.\n","These trajectories will consist roughly of actions, rewards, and states for each step in a trajectory.\n","In the following configuration a single process will be resnonsible for experience gathering, it will sample from 100 environments in parallel entire episodes (so one trajectory will contain 2000 steps) per a phase.\n","There will be 100 * 2000 = 200000 steps per iteration for training available.\n","- During the training phase the PPO algorithm will use the collected experience trajectories to construct a loss function and update weights in the neural net (our trader).\n","The algorithm will also split a batch (200000 steps) to smaller mini-batches (20000 in our case)."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1653803286683,"user":{"displayName":"shamil yakubov","userId":"14689336937540158458"},"user_tz":-240},"id":"pT0DdOeeJaVi"},"outputs":[],"source":["stop_iters = 200\n","n_workers = 1\n","n_envs_per_worker = 100\n","r_fragment_length = 2000\n","train_batch_size = n_workers * n_envs_per_worker * r_fragment_length\n","sgd_minibatch_size = int(train_batch_size / 10)\n","train_dict = prepare_dict(df_train)\n","config = {\n","    \"env\": CryptoEnv,\n","    \"env_config\": {\n","        \"price_array\": train_dict['price_array'],\n","        \"observations\": train_dict['observations'],\n","        \"initial_capital\": 1e4,\n","        \"gamma\": 0.99,\n","        \"max_steps\": r_fragment_length,\n","    },\n","    \"num_gpus\": 1,\n","    \"model\": {\n","        \"vf_share_layers\": False,\n","    },\n","    \"num_workers\": n_workers,\n","    \"num_envs_per_worker\": n_envs_per_worker,\n","    \"rollout_fragment_length\": r_fragment_length,\n","    \"train_batch_size\": train_batch_size,\n","    \"sgd_minibatch_size\": sgd_minibatch_size,\n","    \"batch_mode\": \"complete_episodes\",\n","    \"framework\": \"tf\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7002507,"status":"ok","timestamp":1653810684226,"user":{"displayName":"shamil yakubov","userId":"14689336937540158458"},"user_tz":-240},"id":"3AqE3QHTJaVj","outputId":"a429f0d2-dbea-441f-baad-30f9d904345f"},"outputs":[],"source":["ppo_config = ppo.DEFAULT_CONFIG.copy()\n","ppo_config.update(config)\n","ppo_config[\"lr\"] = 1e-5\n","trainer = ppo.PPOTrainer(config=ppo_config, env=CryptoEnv)\n","for iter in range(stop_iters):\n","    result = trainer.train()\n","    # print(f\"Iteration {iter}.\", end =\"\\r\")\n","    print(f\"Iteration {iter}.\")"]},{"cell_type":"markdown","metadata":{"id":"t1AkuALMJaVj"},"source":["## Visualize results "]},{"cell_type":"markdown","metadata":{},"source":["Let's check the trained agent on the entire training set.\n","Over the whole period, the bitcoin price increases from 38000 usdt to 44000 usdt (by about 16 percent).\n","The total asset received by the agent increases from 8,500 usdt to 14,000 usdt (about 65 percent).\n","Not bad for a tiny network with very simple data preparation and standard hyperparameters trained in so little time.\n","Most valuable, this example shows that reinforcement learning is a workable approach for implementing fully automated trading bots."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":85983,"status":"ok","timestamp":1653810770205,"user":{"displayName":"shamil yakubov","userId":"14689336937540158458"},"user_tz":-240},"id":"8YAzVRKpJaVk","outputId":"217068ee-3ee8-4512-c670-b4d8b9b0e3a7"},"outputs":[],"source":["config[\"env_config\"][\"max_steps\"] = df_train.shape[0] - 1\n","env = CryptoEnv(config[\"env_config\"])\n","actions = []\n","total_assets = []\n","rewards = []\n","observation = env.reset()\n","while True:\n","    action = trainer.compute_action(observation)\n","    observation, reward, done, info = env.step(action)\n","    rewards.append(reward)\n","    total_assets.append(info[\"Final total asset\"])\n","    actions.append(action)\n","    if done:\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRhidpy8JaVk"},"outputs":[],"source":["init_step, final_step = info[\"Initial step\"], info[\"Final step\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"elapsed":1142,"status":"ok","timestamp":1653810771348,"user":{"displayName":"shamil yakubov","userId":"14689336937540158458"},"user_tz":-240},"id":"P9daWv_5JaVk","outputId":"391b5ba7-eb71-47c4-f10e-e9162b48a96d"},"outputs":[],"source":["fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n","ax.plot(df_train.iloc[init_step: final_step].index, df_train.iloc[init_step: final_step]['price'])\n","ax.set_xlabel(\"Time\")\n","ax.set_ylabel(\"Price\")\n","ax.set_title(\"Training set\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":351},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1653810771349,"user":{"displayName":"shamil yakubov","userId":"14689336937540158458"},"user_tz":-240},"id":"CgXZtzvsJaVl","outputId":"0b150ab4-ad7a-40c1-b2ed-7a7011dd313c"},"outputs":[],"source":["fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n","ax.scatter(df_train.iloc[init_step: final_step].index, np.array(total_assets))\n","ax.set_xlabel(\"Time\")\n","ax.set_ylabel(\"Total asset\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1653810771351,"user":{"displayName":"shamil yakubov","userId":"14689336937540158458"},"user_tz":-240},"id":"aL31XZeAJaVm"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"CrypRLAgent.ipynb","provenance":[{"file_id":"https://github.com/shmyak-ai/cryp-bot-demo/blob/main/CrypRLAgent.ipynb","timestamp":1653733370984}],"toc_visible":true},"interpreter":{"hash":"8f46f886b21fe00123f344738b44e766b5b2ce7455afcc0e3672256ebe063244"},"kernelspec":{"display_name":"Python 3.8.13 ('tf')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
